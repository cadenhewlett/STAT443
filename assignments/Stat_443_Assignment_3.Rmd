---
title: "Stat 443: Time Series and Forecasting"
subtitle: "Assignment 3: Time Series Models"
author: Caden Hewlett
header-includes:
    - \usepackage{bbm}
    - \usepackage{yfonts}
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(zoo)
library(tseries)
library(ggplot2)
library(gridExtra)
library(reshape2)
library(forecast)
library(ggfortify)
library(grid)
library(gt)
library(gtExtras)
library(knitr)
# library(mgcv)
library(readr)
library(tidyverse)
library(lubridate)
library(xts)
# tinytex::reinstall_tinytex(repository = "illinois")
```

# Question 1: El Niño Forecasting

The file `NINO34.csv` contains the monthly El Niño 3.4 index from 1870 to 2023. The El Niño 3.4 index represents the average equatorial sea surface temperature (in degrees Celsius) from around the international dateline to the coast of South America. 


## Part a

Perform exploratory data analysis.

### Part a.1

Import the data into R and create a time-series object for the El Niño 3.4 index.

```{r, warning=FALSE}
# import data
setwd('C:/Users/caden/OneDrive/Desktop/STAT_443/STAT443')
df <- read.csv("data/NINO34.csv")

# raw data is in a bad format, need to pivot it without years
dflong <- df %>%
  pivot_longer(cols = -Year, names_to = "Month", values_to = "Value")
# re-translate into dates with month abbreviations
dflong <- dflong %>%
  mutate(Date = make_date(Year, match(Month, month.abb), 1))
# then, format into a time series!
nino_ts <- ts(dflong$Value,
              start = c(1870, 1), frequency = 12)

# time series length = data frame length = 154 years * 12 months per year
stopifnot(all.equal(length(nino_ts), nrow(df)*12, 154*12))
```

Break the time series object into a training and test set. You can use the function
`window()` on a ts object to split the data. Let the training set be from January 1870
to December 2021, and let the test set start in January 2022 and end in November
2023.

```{r}
# split data into train and test
nino_train = window(nino_ts, start = c(1870, 1), end = c(2021, 12))
nino_test = window(nino_ts, start = c(2022, 1), end = c(2023, 11))
# verify split
stopifnot(all.equal(
          length(nino_test)+length(nino_train), length(na.remove(nino_ts))
      )
)  
```


### Part a.2


Plot the training data as well as its acf and pacf. 

\underline{Training Data}

```{r plotnino}
p1data = fortify.zoo(nino_train)
p1 <- ggplot(p1data, aes(x = Index, y = nino_train)) +
  geom_line(color = "#0077b6", linewidth = 0.1) +
  labs(
    title = "El Nino 3.4 Index from Jan. 1870 to Dec. 2021",
    subtitle = expression(
      paste("Index Represents Average Equatorial Sea Surface Temperature (in ", 
          degree, "C)")),
    y = expression(
      paste("Average Equatorial Sea Surface Temperature (in ", degree, "C)")),
    x = "Year"
  ) + theme_bw() +
  theme(panel.grid.minor = element_line(
    color = "grey90",
    linetype = "dashed",
    linewidth = 0.5
  ))
print(p1)
```


\underline{Autocorrelation Function}
<!-- ```{r}  acf(nino_train, lag.max = 36)  ``` -->
```{r acfnino}
p2data = data.frame(
  h = 0:36,
  rh = acf(nino_train, plot = FALSE, lag.max = 36)$acf
)
n = length(nino_train)
p2 <- ggplot(p2data, aes(x = h, y = rh)) +
  geom_hline(yintercept = 2/sqrt(n),
             linetype = "dashed",
             col = "#03045e") +
  geom_hline(yintercept = -2/sqrt(n),
             linetype = "dashed",
             col = "#03045e") +
  ylim(-0.5, 1) +
  geom_segment(aes(xend = h, yend = 0),
               color = "#48cae4",
               linewidth = 1) +
  geom_hline(yintercept = 0,
             linetype = "dashed",
             color = "darkgray") +
  labs(x = "Lag",  y = "ACF Values",
       title = "Correlogram of El Nino 3.4 Index",
       subtitle = "Using Training Data Spanning Jan. 1870 to Dec. 2021") +
    
  theme_bw()
print(p2)
```


\underline{Partial Autocorrelation Function}

```{r pacfnino}
p3data = data.frame(
  h = 1:36,
  rhh = pacf(nino_train, plot = FALSE, lag.max = 36)$acf
)

p3 <- ggplot(p3data, aes(x = h, y = rhh)) +
  geom_hline(yintercept = 2/sqrt(n),
             linetype = "dashed",
             col = "#00b4d8") +
  geom_hline(yintercept = -2/sqrt(n),
             linetype = "dashed",
             col = "#00b4d8") +
  ylim(-0.5, 1) +
  geom_segment(aes(xend = h, yend = 0),
               color = "#023e8a",
               linewidth = 1) +
  geom_hline(yintercept = 0,
             linetype = "dashed",
             color = "darkgray") +
  labs(x = "Lag",  y = "PACF Values",
       title = "Partial ACF of El Nino 3.4 Index",
       subtitle = "Using Training Data Spanning Jan. 1870 to Dec. 2021") +
    
  theme_bw()
print(p3)
```

\underline{Comments}


From the ACF, it is very likely that the original series exhibits a trend. This is due to the fact that there is a clear sinusoidal component of the ACF that is not dampening at a high rate (as we could potentially expect in models with AR components with $\alpha_i < 0$ for $i\in[1,p]$). 

In fact, we can replicate the pattern in the sample ACF quite closely by simply using a sine function alongside random noise. Let $\{Z_t\}_{t = 1}^{500} \overset{\text{iid}}{\sim} N(0, 1)$ and let $X_t = Z_t + \sin(t/2)$, for $t \in \{1, 2, \dots, 500\} \subset \mathbb{N}$. A plot of this artificial additive seasonal model is below:
```{r}
set.seed(443); Z = rnorm(500); t = seq(1:500)
acf(Z + sin(t/2), main = "Artificial ACF", lag.max = 36)
```
As you can see, this artificial ACF closely matches our observed sample ACF from earlier. As such, it is very likely that there exists some seasonal term $s_t$ in the time series of sea surface temperature data. We would intuitively anticipate a seasonal component to these data, as it is very likely that sea temperatures vary over the course of the year due to seasons as measurements are coming from the same area (the international dateline to the coast of South America.)  Moreover, from a more informal observational perspective, a plot of the original series data doesn't seem to support a non-constant seasonal amplitude - as in there's no changing peak heights over time - which may indicate an additive seasonality rather than multiplicative. 


In addition, neither the plot of the data nor the ACF seem to indicate a significant trend in the data. If there were a trend component $m_t$, we would see some consistent change over time in the overall direction of the series beyond simple seasonality. It doesn't seem like there is anything like that in this series; **however**, a purely visual analysis isn't comprehensive so this doesn't mean that a trend component doesn't exist. 


Finally, to determine whether or not the series is stationary, we recall the definition of a weakly stationary stochastic process. Specifically, we will consider the first property of weak stationarity - that the mean is constant. We defined this formally in Assignment 1 previously, and was given similar to the following:
$$
\text{Weak Stationarity Property One: }\; \exists \mu \in \mathbb{R} \text{ s.t. } \forall t \in \mathbb{Z}, \; \mathbb{E}(X_t) = \mu
$$
The presence of *either* a seasonal component or a trend causes a contradiction, as the expected value of the series becomes some function of $t$ (and hence cannot be a constant.) 

Thus, for our particular series, we have the following implication, letting $s_t$ be the seasonal component, $c \in \mathbb{R}$ be some real constant and $f(t)$ be a non-constant real function (as in, it changes with $t$.) Then, 
$$
\exists s_t \implies  \forall t \;\mathbb{E}(X_t) = f(t) + c \; \implies \not\exists \mu \in \mathbb{R} \text{ s.t. } \forall t\; \mathbb{E}(X_t) = \mu \;\, \therefore \;X_t \text{ is not stationary.} \hspace{0.25cm} \square
$$
 In short, due to the presence of a trend, the time series for the training data is not stationary. It should be noted that not much can be concluded with results from the PACF, as the series is non-stationary. 


## Part b

Forecast sea surface temperature for 2022 and 2023 using the Box-Jenkins method and the data from 1870-2021.

### Part b.1.

Remove any seasonal variation and trend from the training data, if there is any, using the `stl` function in R. 

Plot the filtered data set, as well as its acf and pacf.


In experimenting with only de-seasonalizing and only de-trending the data, or both, the best performance (i.e. stationary/near stationary in the ACF) came from removing both season and trend.

```{r deseason}
nino_stl = stl(nino_train, s.window = "periodic")
loess_decomp = data.frame(nino_stl$time.series) 
nino_filtered =  nino_train - loess_decomp$seasonal # - loess_decomp$trend
acf_filtered = acf(nino_filtered, lag.max = 30, plot = F)$acf
```
```{r}
pacf_filtered = pacf(nino_filtered, lag.max = 30, plot = F)$acf
```
```{r}
# plot(nino_stl)
```

ideas:

```{r}
# fit1 = arima(nino_filtered, order = c(6, 0, 0)) # -278.9
# fit2 = arima(nino_filtered, order = c(17, 0, 0)) # -278.9
# arima(nino_filtered, order = c(0, 0, 12))
# ?arima
```

```{r}
# fit2$loglik
```

### Bonus: Stepwise Model Optimization According to AIC

If we wanted to, we can also use the `auto.arima` function from the `forecast` package to pick the best model according to AIC. The function performs stepwise non-approximation for AIC and returns the model corresponding to the minimum. It takes a bit of time to run (since no approxiamtions are done.)
```{r testzone}
# bonus_model = auto.arima(nino_train, ic = "aic", stepwise = TRUE, approximation = FALSE)
```


## Part c


Forecast sea surface temperature for 2022 and 2023 using the Holt-Winters method and the data from 1870-2021. 


### Part c.1.

Use the HoltWinters function in R to fit an appropriate model to the training data. 


As in our original series (and with the Box-Jenkins method) we did not observe a significant trend, we will construct a Holt-Winters model with $\beta = 0$.


We will use an additive seasonal effect, which inherits the `Frequency` of the time series as the period. In this case, then, $p=12$.

We then have an additive seasonal effect $I_t$ for $t$ given by the following:
$$
{I}_{t} = \gamma(x_t - {L}_t) + (1 -  \gamma){I}_{t - p}
$$

Where ${L}_t$ is the level component that defines Holt-Withers smoothing/forecasting methods. Since $\beta = 0$, it is constructed without a $T$ component, and is given by:
$$
{L}_t = \alpha(x_t - {I}_{t-p}) + (1 - \alpha){L}_{t-1}
$$
For $\alpha, \gamma \in [0,1]$. Further, for $\ell \in \mathbb{Z}$, the $\ell$-step ahead forecast at time $t$ is given by the following.
$$
\hat{x}_t (\ell) = {L}_t + {I}_{t-p+\ell}
$$
We report the coefficients from `HoltWinters` in the table below.
```{r}
hw_no_trend = HoltWinters(nino_train, beta = 0, seasonal = "additive") # no trend
hw_results_coefs = (data.frame(
  Alpha = round(hw_no_trend$alpha, 3),
  Beta = paste(0, ".000", sep = ""),
  Gamma = paste(hw_no_trend$gamma, ".000", sep = "")
))
rownames(hw_results_coefs) = NULL
kable(hw_results_coefs, caption = "Holt-Winters Coefficients")
```
Our fitted $\hat\gamma = 1$, implying that the model places maximum smoothing weight on the current state of the season (i.e. the $(1 - \gamma){I}_{t-p}$ component has zero weight according to the fitting process.) Further, our fitted $\hat\alpha \approx 0.96$ - this means that the model is placing a lot of significance on the current level (and period) and much less on levels in the past when fitting.


### Part c.2.


Use this model to predict sea surface temperature from Jan 2022 through Nov 2023. 

```{r}
hw_predictions = data.frame(
  predict(hw_no_trend, n.ahead = 23, prediction.interval = TRUE, level = 0.95))

# prepare the hw forecast data
p4data = data.frame(
  Time = as.Date(time(nino_test)),
  Observed = as.numeric(nino_test),
  Forecast = as.numeric(hw_predictions$fit),
  Lower = as.numeric(hw_predictions$lwr),
  Upper = as.numeric(hw_predictions$upr)
)
```
```{r fancyplot}
# make fancy as heck plot
p4 = ggplot(data = p4data, aes(x = Time)) +
  # lines for obsv, pred and interval bounds
  geom_line(aes(y = Observed, color = "Actual"),  lwd = 1) +
  geom_line(aes(y = Forecast, color = "Forecast"),lwd = 1) +
  geom_line(aes(y = Lower, color = "95% Interval"), alpha = 0.75) +
  geom_line(aes(y = Upper, color = "95% Interval"), alpha = 0.75) +
  # light blue fill area between prediction intervals
  geom_ribbon(aes(ymin = Lower, ymax = Upper), fill = "#caf0f8", alpha = 0.15) +
  # legend and colour assignment
  scale_color_manual(name = "",
    values = c(
    "Actual" = "#696969",
    "Forecast" = "#02a9ea",
    "95% Interval" = "#9fc8d6"
  )) +
  # customize x-axis for nice dates
  scale_x_date(date_breaks = "4 month",  date_labels = "%b, %Y") +
  # add titles with units
  labs(
    title = "Forecast and 95% Prediction Interval of Test Data",
    subtitle = "El Nino 3.4 Index from Jan. 2022 to Nov. 2023",
    x = "Month and Year",
    y = expression(
      paste("Average Equatorial Sea Surface Temperature (in ", degree, "C)")
    )
  ) +
  theme_bw() +
  theme(panel.grid.minor =  element_line(
    color = "grey90",
    linewidth  = 0.35,
    linetype = "dashed"
  ), legend.position = "top", legend.justification = "left", 
  legend.margin = margin(0,0,0,0))
print(p4)
```


### Part c.3.

Calculate the mean squared prediction error.

The mean squared prediction error for our $N_{\ell} = 23$ total $\ell$ step ahead forecast is given by:
$$
\text{MSE}_{\text{pred}} = \frac{1}{N}\sum_{\text{Holdout}}\Big(\text{Truth} - \text{Forecast} \Big)^2 = \frac{1}{N_{\ell}}\sum_{\ell = 1}^{N_{\ell}} \Big( x_{t + \ell} - \hat{x}_t(\ell)\Big)^2
$$

```{r}
N = length(nino_test)
# verify equality in set cardinalities
stopifnot(all.equal(N, length(hw_predictions$fit)))
# calculate ms prediction error
ms_pre_hw = mean( (hw_predictions$fit - nino_test)^2 )
ms_pre_hw
```


### Part c.4.

How does it compare to the Box-Jenkins models above?

```{r}
# TODO: Comparison code goes here...
```



# Question 2 : Hours Worked Forecasting

In this question we will predict the time series of monthly average values of the usual hours worked across all industries in Canada for the period from January 1987 until December 2023, which was explored in Assignment 1, using the file `usual_hours_worked_ca.csv`. 

We'll use the Box-Jenkins method and Holt-Winters method.


## Part 1: Data Preparation

Read in the data and create a time-series object for the mean monthly working hours:

```{r}
hDF = read.csv("usual_hours_worked_ca.csv")
hours_series = ts(hDF$Hours, start = c(1987, 1), frequency = 12)
```

Separate the data into train and test. The training dataset should include all observations up to and including December
2020; The test dataset should include all observations from January 2021 to December 2023

```{r}
hours_train = window(hours_series, end = c(2020, 12))
hours_test = window(hours_series, start = c(2021, 1), end = c(2023, 12))
# verify split is valid
stopifnot(all.equal(length(hours_test)+length(hours_train),
                    length(na.remove(hours_series))))
```

Plot the training data.

```{r}
p5data = fortify.zoo(hours_train)
n = length(hours_train)
p5 <- ggplot(p5data, aes(x = Index, y = hours_train)) +
  geom_line(color = "#410B13", linewidth = 0.1) +
  labs(
    title = "Monthly Average of Usual Hours Worked in Canada",
    subtitle = "Across all Industries from January 1987 to December 2020",
    y = "Monthly Mean Working Time (Hours)",
    x = "Year"
  ) + theme_bw() +
  theme(panel.grid.minor = element_line(
    color = "grey90",
    linetype = "dashed",
    linewidth = 0.5
  ))
print(p5)
```

## Part 2: Box-Jenkins Method

In this part, we select and fit a $\text{SARIMA}(p, d, q) \times (P, D, Q)_s$ model and make forecasts using the fitted model.

### Part A: Differencing

Difference the training set time series at lag 1. 
```{r}
hours_train_diff = diff(hours_train, lag = 1, differences = 1)
```
Plot the new time series and its correlogram. 

\underline{New Series}
```{r}
p6data = fortify.zoo(hours_train_diff)
p6 <- ggplot(p6data, aes(x = Index, y = hours_train_diff)) +
  geom_line(color = "#BA1F33", linewidth = 0.1) +
  labs(
    title = "Differenced Series of Monthly Average Hours Worked in Canada",
    subtitle = "Across all Industries from January 1987 to December 2020",
    y = "Monthly Mean Working Time (Hours)",
    x = "Year"
  ) + theme_bw() +
  theme(panel.grid.minor = element_line(
    color = "grey90",
    linetype = "dashed",
    linewidth = 0.5
  ))
print(p6)
```

\underline{Correlogram}

```{r}
p7data = data.frame(
  h = 0:36,
  rh = acf(hours_train_diff, plot = FALSE, lag.max = 36)$acf
)
p7 <- ggplot(p7data, aes(x = h, y = rh)) +
  geom_hline(yintercept = 2/sqrt(n),
             linetype = "dashed",
             col = "#421820") +
  geom_hline(yintercept = -2/sqrt(n),
             linetype = "dashed",
             col = "#421820") +
  ylim(-0.6, 1) +
  geom_segment(aes(xend = h, yend = 0),
               color = "#CD5D67",
               linewidth = 1) +
  geom_hline(yintercept = 0,
             linetype = "dashed",
             color = "darkgray") +
  labs(x = "Lag",  y = "ACF Values",
       title = "Correlogram of Differenced Series of Monthly Average Work Hours",
       subtitle = "Using Training Data for Canada, Jan. 1987 to Dec. 2020") +
    
  theme_bw()
print(p7)
```

\underline{Comment on what you observe.}

While the single-iteration ($d = 1$) of sequential differencing at lag 1 appears to have helped to mitigate the trend in the original training data, as seen in the plot of the differenced training time series, there still appears to be a notable seasonal component in the data. We know that this is likely the case due to the sinusoidal component in the correlogram, which is not really decreasing as a function of lag as we would like to see. This periodicity is indicative of a lingering seasonal component that we still need to difference out or remove via other methods.


### Part B: Seasonal Differencing

Apply seasonal differencing to remove seasonal variation. Since we have monthly data, $s = 12$ seems sensible. 

```{r}
hours_train_diff_s = diff(hours_train_diff, lag = 12, differences = 1)
```

Plot the resulting differenced time series along with its sample acf and pacf. 

```{r}

acf(hours_train_diff_s, lag.max = 36) # MA at 12, Q = 1, MA at 1, q = 1 
pacf(hours_train_diff_s, lag.max = 96) # PACF doesn't really show a 
test = pacf(hours_train_diff_s, lag.max = 96, plot = F)$acf # P = 12 perhaps
# which( abs(test) > 2/sqrt(n) )
```


Comment on what you observe

```{r}
auto.arima(hours_train)
```

### Part C: Difference Parameters.
Based on the results of Part II (a) and (b), specify the values of $d$, $D$, and $s$.


### Part D: SARIMA Parameters

Based on the plots in Part II (b), suggest possible values of $p$, $P$, $q$, and $Q$, justifying your choices.


### Part E: Iterative AIC

Now use the Akaike’s Information Criterion (AIC) to select the model based on the training dataset in Part I. Fix the values of p and P as your suggestions in Part II (d),


Consider $q \in [0, 5]$ and $Q \in [0, 5]$.

Select the values of q and Q according to the AIC values. 

Fit the model you choose and print the values of the estimated parameters along with the AIC value for the model.


First, we declare our fixed $p$, $P$, $s$, $d$ and $D$ values, then iterate through the different 25 models that can be developed and report the AIC for each. 

```{r, all_AIC}
p = 0; P = 1; s = 12; d = 1; D = 1
# declare empty matrix
results = matrix(0, nrow = 5, ncol = 5)
# with legible names
rownames(results) = c("q=1", "q=2","q=3","q=4","q=5")
colnames(results) = c("Q=1", "Q=2","Q=3","Q=4","Q=5")
# iterate through all candidates
for (q in 1:5){
  for (Q in 1:5){
    results[q, Q] = arima(hours_train,
                          order = c(p, d, q),
                          seasonal = c(P, D, Q))$aic
  }
}

```

The AIC results are summarized in the table below.
```{r}
kable(round(results, 3), caption = "AIC Values for Varying (q, Q)")
P
```

According to this table, the best model corresponds to:
```{r}
which(results == min(results), arr.ind = TRUE)
```
Which corresponds to $q = 4$ and $Q = 5$. It has an AIC of $-627.61$

The full model is given below:
```{r}
best_fit = arima(hours_train, order = c(0, 1, 4), seasonal = c(1, 1, 5))
best_fit
```

```{r}
best_fit$coef
```

