---
title: "Stat 443: Time Series and Forecasting"
subtitle: "Assignment 3: Time Series Models"
author: Caden Hewlett
header-includes:
    - \usepackage{bbm}
    - \usepackage{amsmath}
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(zoo)
library(tseries)
library(ggplot2)
library(gridExtra)
library(reshape2)
library(forecast)
library(ggfortify)
library(grid)
library(gt)
library(gtExtras)
library(knitr)
# library(mgcv)
library(readr)
library(tidyverse)
library(lubridate)
library(xts)
library(kableExtra)
```

# Question 1: El Niño Forecasting

The file `NINO34.csv` contains the monthly El Niño 3.4 index from 1870 to 2023. The El Niño 3.4 index represents the average equatorial sea surface temperature (in degrees Celsius) from around the international dateline to the coast of South America. 


## Part a

Perform exploratory data analysis.

### Part a.1

Import the data into R and create a time-series object for the El Niño 3.4 index.

```{r, warning=FALSE}
# import data
# setwd('C:/Users/caden/OneDrive/Desktop/STAT_443/STAT443')
df <- read.csv("NINO34.csv")

# raw data is in a bad format, need to pivot it without years
dflong <- df %>%
  pivot_longer(cols = -Year, names_to = "Month", values_to = "Value")
# re-translate into dates with month abbreviations
dflong <- dflong %>%
  mutate(Date = make_date(Year, match(Month, month.abb), 1))
# then, format into a time series!
nino_ts <- ts(dflong$Value,
              start = c(1870, 1), frequency = 12)

# time series length = data frame length = 154 years * 12 months per year
stopifnot(all.equal(length(nino_ts), nrow(df)*12, 154*12))
```

Break the time series object into a training and test set. You can use the function
`window()` on a ts object to split the data. Let the training set be from January 1870
to December 2021, and let the test set start in January 2022 and end in November
2023.

```{r}
# split data into train and test
nino_train = window(nino_ts, start = c(1870, 1), end = c(2021, 12))
nino_test = window(nino_ts, start = c(2022, 1), end = c(2023, 11))
# verify split
stopifnot(all.equal(
          length(nino_test)+length(nino_train), length(na.remove(nino_ts))
      )
)  
```


### Part a.2


Plot the training data as well as its acf and pacf. 

\underline{Training Data}

```{r plotnino}
p1data = fortify.zoo(nino_train)
```
```{r, include=FALSE}
p1 <- ggplot(p1data, aes(x = Index, y = nino_train)) +
  geom_line(color = "#0077b6", linewidth = 0.1) +
  labs(
    title = "El Nino 3.4 Index from Jan. 1870 to Dec. 2021",
    subtitle = expression(
      paste("Index Represents Average Equatorial Sea Surface Temperature (in ", 
          degree, "C)")),
    y = expression(
      paste("Average Equatorial Sea Surface Temperature (in ", degree, "C)")),
    x = "Year"
  ) + theme_bw() +
  theme(panel.grid.minor = element_line(
    color = "grey90",
    linetype = "dashed",
    linewidth = 0.5
  ))
```
```{r}
print(p1)
```

\underline{Autocorrelation Function}
<!-- ```{r}  acf(nino_train, lag.max = 36)  ``` -->
```{r acfnino}
p2data = data.frame(
  h = 0:36,
  rh = acf(nino_train, plot = FALSE, lag.max = 36)$acf
)
```
```{r, include = FALSE}
n = length(nino_train)
p2 <- ggplot(p2data, aes(x = h, y = rh)) +
  geom_hline(yintercept = 2/sqrt(n),
             linetype = "dashed",
             col = "#03045e") +
  geom_hline(yintercept = -2/sqrt(n),
             linetype = "dashed",
             col = "#03045e") +
  ylim(-0.5, 1) +
  geom_segment(aes(xend = h, yend = 0),
               color = "#48cae4",
               linewidth = 1) +
  geom_hline(yintercept = 0,
             linetype = "dashed",
             color = "darkgray") +
  labs(x = "Lag",  y = "ACF Values",
       title = "Correlogram of El Nino 3.4 Index",
       subtitle = "Using Training Data Spanning Jan. 1870 to Dec. 2021") +
    
  theme_bw()
```
```{r}
print(p2)
```

\underline{Partial Autocorrelation Function}

```{r pacfnino}
p3data = data.frame(
  h = 1:36,
  rhh = pacf(nino_train, plot = FALSE, lag.max = 36)$acf
)
```
```{r, include=FALSE}
p3 <- ggplot(p3data, aes(x = h, y = rhh)) +
  geom_hline(yintercept = 2/sqrt(n),
             linetype = "dashed",
             col = "#00b4d8") +
  geom_hline(yintercept = -2/sqrt(n),
             linetype = "dashed",
             col = "#00b4d8") +
  ylim(-0.5, 1) +
  geom_segment(aes(xend = h, yend = 0),
               color = "#023e8a",
               linewidth = 1) +
  geom_hline(yintercept = 0,
             linetype = "dashed",
             color = "darkgray") +
  labs(x = "Lag",  y = "PACF Values",
       title = "Partial ACF of El Nino 3.4 Index",
       subtitle = "Using Training Data Spanning Jan. 1870 to Dec. 2021") +
    
  theme_bw()
```
```{r}
print(p3)
```

\underline{Comments}


From the ACF, it is very likely that the original series exhibits a trend. This is due to the fact that there is a clear sinusoidal component of the ACF that is not dampening at a high rate (as we could potentially expect in models with AR components with $\alpha_i < 0$ for $i\in[1,p]$). 

In fact, we can replicate the pattern in the sample ACF quite closely by simply using a sine function alongside random noise. Let $\{Z_t\}_{t = 1}^{500} \overset{\text{iid}}{\sim} N(0, 1)$ and let $X_t = Z_t + \sin(t/2)$, for $t \in \{1, 2, \dots, 500\} \subset \mathbb{N}$. A plot of this artificial additive seasonal model is below:
```{r}
set.seed(443); Z = rnorm(500); t = seq(1:500)
acf(Z + sin(t/2), main = "Artificial ACF", lag.max = 36)
```
As you can see, this artificial ACF closely matches our observed sample ACF from earlier. As such, it is very likely that there exists some seasonal term $s_t$ in the time series of sea surface temperature data. We would intuitively anticipate a seasonal component to these data, as it is very likely that sea temperatures vary over the course of the year due to seasons as measurements are coming from the same area (the international dateline to the coast of South America.)  Moreover, from a more informal observational perspective, a plot of the original series data doesn't seem to support a non-constant seasonal amplitude - as in there's no changing peak heights over time - which may indicate an additive seasonality rather than multiplicative. 


In addition, neither the plot of the data nor the ACF seem to indicate a significant trend in the data. If there were a trend component $m_t$, we would see some consistent change over time in the overall direction of the series beyond simple seasonality. It doesn't seem like there is anything like that in this series; **however**, a purely visual analysis isn't comprehensive so this doesn't mean that a trend component doesn't exist. 


Finally, to determine whether or not the series is stationary, we recall the definition of a weakly stationary stochastic process. Specifically, we will consider the first property of weak stationarity - that the mean is constant. We defined this formally in Assignment 1 previously, and was given similar to the following:
$$
\text{Weak Stationarity Property One: }\; \exists \mu \in \mathbb{R} \text{ s.t. } \forall t \in \mathbb{Z}, \; \mathbb{E}(X_t) = \mu
$$
The presence of *either* a seasonal component or a trend causes a contradiction, as the expected value of the series becomes some function of $t$ (and hence cannot be a constant.) 

Thus, for our particular series, we have the following implication, letting $s_t$ be the seasonal component, $c \in \mathbb{R}$ be some real constant and $f(t)$ be a non-constant real function (as in, it changes with $t$.) Then, 
$$
\exists s_t \implies  \forall t \;\mathbb{E}(X_t) = f(t) + c \; \implies \not\exists \mu \in \mathbb{R} \text{ s.t. } \forall t\; \mathbb{E}(X_t) = \mu \;\, \therefore \;X_t \text{ is not stationary.} \hspace{0.25cm} \square
$$
 In short, due to the presence of a trend, the time series for the training data is not stationary. It should be noted that not much can be concluded with results from the PACF, as the series is non-stationary. 


## Part b

Forecast sea surface temperature for 2022 and 2023 using the Box-Jenkins method and the data from 1870-2021.

### Part b.1.

Remove any seasonal variation and trend from the training data, if there is any, using the `stl` function in R. 

We define an object `nino_stl` using `loess` decomposition, and plot each of S, T and L (seasonal, trend and irregular) of the original series below:

```{r}
# perform loess decomposition
nino_stl = stl(nino_train, s.window = "periodic")
# cast to data frame
decomposition = as.data.frame(nino_stl$time.series)
decomposition$time = as.numeric(time(nino_train))
```
Note that the code to plot the decomposition with ggplot became quite long, so I made the code cell hidden. The complete plots are provided below:
```{r, include=FALSE}
# plot all of them (each slightly different so can't make a function)
pb1 <- ggplot(decomposition, aes(x = time, y = seasonal)) + 
  geom_line(color = "#cdb4db") + 
  ggtitle("Seasonal Component") + 
  labs(y = "Seasonal", subtitle = "Using El Nino Training Data Spanning Jan. 1870 to Dec. 2021") +
  theme_bw() +
  theme(panel.grid.minor = element_line(color = "grey90", linetype = "dashed", linewidth = 0.5),
        axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank())

pb2 <- ggplot(decomposition, aes(x = time, y = trend)) + 
  geom_line(color = "#ffafcc") + 
  ggtitle("Trend Component") + 
  labs(y = "Trend") +
  theme_bw() +
  theme(panel.grid.minor = element_line(color = "grey90", linetype = "dashed", linewidth = 0.5),
        axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank())

pb3 <- ggplot(decomposition, aes(x = time, y = remainder)) + 
  geom_line(color = "#a2d2ff") + 
  ggtitle("Remainder Component") + 
  labs(y = "Remainder") +
  theme_bw() +
  theme(panel.grid.minor = element_line(color = "grey90", linetype = "dashed", linewidth = 0.5))
```
```{r}
grid.arrange(pb1, pb2, pb3, ncol = 1)
```



Now, we plot the filtered data set, as well as its acf and pacf. It doesn't seem like there is a significant trend component, as our original observations also showed, so we merely de-seasonalize the data.  The removal of $s_t$ and the plot of the filtered data is below:


\underline{Filtered Data}
```{r deseason}
# remove the season
nino_filtered =  nino_train - decomposition$seasonal
# plot the filtered series
pb4data = fortify.zoo(nino_filtered)
pb4 <- ggplot(pb4data, aes(x = Index, y = nino_filtered)) +
  geom_line(color = "#b56576", linewidth = 0.1) +
  labs(
    title = "Deseasonalized El Nino 3.4 Index from Jan. 1870 to Dec. 2021",
    subtitle = "Seasonal Component Identified via Loess Decomposition",
    y = expression(
      paste("Average Equatorial Sea Surface Temperature (in ", degree, "C)")),
    x = "Year"
  ) + theme_bw() +
  theme(panel.grid.minor.y = element_line(
    color = "grey90",
    linetype = "dashed",
    linewidth = 0.5
  ))
print(pb4)
```
\underline{Autocorrelation}

```{r}
p5bdata = data.frame(
  h = 0:45,
  rh = acf(nino_filtered, plot = FALSE, lag.max = 45)$acf
)

pb5 <- ggplot(p5bdata, aes(x = h, y = rh)) +
  geom_hline(yintercept = 2/sqrt(n),
             linetype = "dashed",
             col = "#415d43") +
  geom_hline(yintercept = -2/sqrt(n),
             linetype = "dashed",
             col = "#415d43") +
  ylim(-0.2, 1) +
  geom_segment(aes(xend = h, yend = 0),
               color = "#a1cca5",
               linewidth = 1) +
  geom_hline(yintercept = 0,
             linetype = "dashed",
             color = "darkgray") +
  labs(x = "Lag",  y = "PACF Values",
       title = "Sample ACF of Deseasonalized El Nino 3.4 Index",
       subtitle = "Using Training Data Spanning Jan. 1870 to Dec. 2021") +
    
  theme_bw()
print(pb5)
```
\underline{Partial Autocorrelation}

```{r}
p6bdata = data.frame(
  h = 1:45,
  rhh = pacf(nino_filtered, plot = FALSE, lag.max = 45)$acf
)
```
```{r, include= FALSE}
pb6 <- ggplot(p6bdata, aes(x = h, y = rhh)) +
  geom_hline(yintercept = 2/sqrt(n),
             linetype = "dashed",
             col = "#a1cca5") +
  geom_hline(yintercept = -2/sqrt(n),
             linetype = "dashed",
             col = "#a1cca5") +
  ylim(-0.2, 1) +
  geom_segment(aes(xend = h, yend = 0),
               color = "#415d43",
               linewidth = 1) +
  geom_hline(yintercept = 0,
             linetype = "dashed",
             color = "darkgray") +
  labs(x = "Lag",  y = "PACF Values",
       title = "Partial ACF of Deseasonalized El Nino 3.4 Index",
       subtitle = "Using Training Data Spanning Jan. 1870 to Dec. 2021") +
  theme_bw()
```
```{r}
print(pb6)
```

### Part b.2.

In the work to follow, we assume the de-seasonalized time series is stationary, such that the ACF and PACF can be used for model identification.


Now, using the standard graphical tools, let's select two models from the pure MA and pure AR family of models for the filtered data set. 


Since the ACF follows a (slightly complicated) exponentially decaying sinusoidal pattern, it seems quite likely that the best candidate models will be from the AR family. If this were a pure MA process for some order $q$, we would see the ACF sharply cut off after the value of $q$ is reached. Instead, the ACF slowly decays over time; this is indicative of an AR component to the series. Hence, we'll consider models form the AR family of models.


Now, given that we are attempting to find $p$ under the presumption that the series follows some \underline{pure} AR process, we can utilize the PACF to attempt to find a suitable $p$. Unfortunately, it isn't immediately evident which term $p$ in the PACF satisfies the condition we commonly use that $\forall p > k, \;|\hat\alpha_{kk}| < 2 / \sqrt{n}.$ 


We will consider two potential candidates, the first being AR$(1)$ process. The justification for deciding upon $p = 1$ is based on the fact that $\hat\alpha_{11}$ is significantly larger than all other PACF values. While it is true that $\exists k > 1 \text{ s. t. }  |\hat\alpha_{kk}| \geq 2 / \sqrt{n}$, there are no other values in the PACF greater than $\hat\alpha_{11}$. In other words, the PACF may not decline into noise; however, 
$\forall k \in [2, n] \subset \mathbb{Z}, \hat\alpha_{11} > \hat\alpha_{kk}$.
So, while the PACF doesn't cut off into white noise after $p=1$, it does cut off to significantly smaller values.


The second candidate is an AR$(6)$ process. The reasoning is due to the fact that after $\hat\alpha_{66}$, the vast majority of observed $\hat\alpha_{kk}$ values for $k > 6$ seem to be below the white noise threshold. Critically, there isn't an easily distinguishable pattern to those $\hat\alpha_{kk}$ values greater than $\pm \sqrt{2}/n$ after PACF lag 6. This could mean that for $k > 6$ there is simply noise (as we want), however the data could have higher noise values than those commonly assumed by the W.N. threshold. It's important to recall that $\pm \sqrt{2}/n$ is more of a "rule of thumb" rather than a strict rule. 


With all of this mind, we fit the models below:
```{r arvsar}
# ar(1) vs. ar(6)
ar1 = arima(nino_filtered, order = c(1, 0, 0))
ar6 = arima(nino_filtered, order = c(6, 0, 0))
```
Then, we extract the coefficients and show them in the table below, rounding each to three decimal places:
```{r varcolumn_names, include=FALSE}
var_col_names =  c("$\\hat{\\mu}$", "$\\hat{\\alpha}_1$", "$\\hat{\\alpha}_2$", "$\\hat{\\alpha}_3$",
                    "$\\hat{\\alpha}_4$", "$\\hat{\\alpha}_5$", "$\\hat{\\alpha}_6$", "$\\hat{\\sigma}^2$")
```
```{r}
ar_fits = t(round(data.frame(
  AR1 = c(ar1$coef[-1], head(ar1$coef, -1), rep(NA, times = 5), ar1$sigma2),
  AR6 = c(tail(ar6$coef, 1), head(ar6$coef, -1), ar6$sigma2)), 3))
# report with nice latex column names
kable(ar_fits, "latex", escape = FALSE,  col.names = var_col_names)  %>%
  kable_styling(latex_options = "hold_position") %>%
  kable_styling(position = "center")
```


### Part b.3.


Compare the AIC values of your two models. Which would you pick based on AIC?

### Part b.4.


Use the `tsdiag` function in R to plot diagnostics for your two models. What do you
observe?


### Part b.5.


Predict the sea surface temperature for Jan 2022 through Nov 2023 using both
candidate models. Calculate the mean squared prediction error (MSPE) for both
models. Which method performs better?


### Part b.6.

On a single plot, display the test set of sea surface temperature, the predictions for
both models, and their approximate 95% prediction intervals.



## Part c


Forecast sea surface temperature for 2022 and 2023 using the Holt-Winters method and the data from 1870-2021. 


### Part c.1.

Use the HoltWinters function in R to fit an appropriate model to the training data. 


As in our original series (and with the Box-Jenkins method) we did not observe a significant trend, we will construct a Holt-Winters model with $\beta = 0$.


We will use an additive seasonal effect, which inherits the `Frequency` of the time series as the period. In this case, then, $p=12$.

We then have an additive seasonal effect $I_t$ for $t$ given by the following:
$$
{I}_{t} = \gamma(x_t - {L}_t) + (1 -  \gamma){I}_{t - p}
$$

Where ${L}_t$ is the level component that defines Holt-Withers smoothing/forecasting methods. Since $\beta = 0$, it is constructed without a $T$ component, and is given by:
$$
{L}_t = \alpha(x_t - {I}_{t-p}) + (1 - \alpha){L}_{t-1}
$$
For $\alpha, \gamma \in [0,1]$. Further, for $\ell \in \mathbb{Z}$, the $\ell$-step ahead forecast at time $t$ is given by the following.
$$
\hat{x}_t (\ell) = {L}_t + {I}_{t-p+\ell}
$$
We report the coefficients from `HoltWinters` in the table below.
```{r}
hw_no_trend = HoltWinters(nino_train, beta = FALSE, seasonal = "additive") # no trend
hw_results_coefs = (data.frame(
  Alpha = round(hw_no_trend$alpha, 3),
  Beta = paste(0, ".000", sep = ""),
  Gamma = paste(hw_no_trend$gamma, ".000", sep = "")
))
rownames(hw_results_coefs) = NULL
kable(hw_results_coefs, caption = "Holt-Winters Coefficients")
```
Our fitted $\hat\gamma = 1$, implying that the model places maximum smoothing weight on the current state of the season (i.e. the $(1 - \gamma){I}_{t-p}$ component has zero weight according to the fitting process.) Further, our fitted $\hat\alpha \approx 0.96$ - this means that the model is placing a lot of significance on the current level (and period) and much less on levels in the past when fitting.


### Part c.2.


Use this model to predict sea surface temperature from Jan 2022 through Nov 2023. 

```{r}
hw_predictions = data.frame(
  predict(hw_no_trend, n.ahead = 23, prediction.interval = TRUE, level = 0.95))

# prepare the hw forecast data
p4data = data.frame(
  Time = as.Date(time(nino_test)),
  Observed = as.numeric(nino_test),
  Forecast = as.numeric(hw_predictions$fit),
  Lower = as.numeric(hw_predictions$lwr),
  Upper = as.numeric(hw_predictions$upr)
)
```
```{r fancyplot}
# make fancy as heck plot
p4 = ggplot(data = p4data, aes(x = Time)) +
  # lines for obsv, pred and interval bounds
  geom_line(aes(y = Observed, color = "Actual"),  lwd = 1) +
  geom_line(aes(y = Forecast, color = "Forecast"),lwd = 1) +
  geom_line(aes(y = Lower, color = "95% Interval"), alpha = 0.75) +
  geom_line(aes(y = Upper, color = "95% Interval"), alpha = 0.75) +
  # light blue fill area between prediction intervals
  geom_ribbon(aes(ymin = Lower, ymax = Upper), fill = "#caf0f8", alpha = 0.15) +
  # legend and colour assignment
  scale_color_manual(name = "",
    values = c(
    "Actual" = "#696969",
    "Forecast" = "#02a9ea",
    "95% Interval" = "#9fc8d6"
  )) +
  # customize x-axis for nice dates
  scale_x_date(date_breaks = "4 month",  date_labels = "%b, %Y") +
  # add titles with units
  labs(
    title = "Forecast and 95% Prediction Interval of Test Data",
    subtitle = "El Nino 3.4 Index from Jan. 2022 to Nov. 2023",
    x = "Month and Year",
    y = expression(
      paste("Average Equatorial Sea Surface Temperature (in ", degree, "C)")
    )
  ) +
  theme_bw() +
  theme(panel.grid.minor =  element_line(
    color = "grey90",
    linewidth  = 0.35,
    linetype = "dashed"
  ), legend.position = "top", legend.justification = "left", 
  legend.margin = margin(0,0,0,0))
print(p4)
```


### Part c.3.

Calculate the mean squared prediction error.

The mean squared prediction error for our $N_{\ell} = 23$ total $\ell$ step ahead forecast is given by:
$$
\text{MSE}_{\text{pred}} = \frac{1}{N}\sum_{\text{Holdout}}\Big(\text{Truth} - \text{Forecast} \Big)^2 = \frac{1}{N_{\ell}}\sum_{\ell = 1}^{N_{\ell}} \Big( x_{t + \ell} - \hat{x}_t(\ell)\Big)^2
$$

```{r}
N = length(nino_test)
# verify equality in set cardinalities
stopifnot(all.equal(N, length(hw_predictions$fit)))
# calculate ms prediction error
ms_pre_hw = sum ( (nino_test - hw_predictions$fit )^2 ) /23
ms_pre_hw
```
The observed Mean Square prediction error for the Holt-Withers forecast is approximately $1.972$.


### Part c.4.

How does it compare to the Box-Jenkins models above?

```{r}
# TODO: Comparison code goes here...
```



# Question 2 : Hours Worked Forecasting

In this question we will predict the time series of monthly average values of the usual hours worked across all industries in Canada for the period from January 1987 until December 2023, which was explored in Assignment 1, using the file `usual_hours_worked_ca.csv`. 

We'll use the Box-Jenkins method and Holt-Winters method.


## Part 1: Data Preparation

Read in the data and create a time-series object for the mean monthly working hours:

```{r}
hDF = read.csv("usual_hours_worked_ca.csv")
hours_series = ts(hDF$Hours, start = c(1987, 1), frequency = 12)
```

Separate the data into train and test. The training dataset should include all observations up to and including December
2020; The test dataset should include all observations from January 2021 to December 2023

```{r}
hours_train = window(hours_series, end = c(2020, 12))
hours_test = window(hours_series, start = c(2021, 1), end = c(2023, 12))
# verify split is valid
stopifnot(all.equal(length(hours_test)+length(hours_train),
                    length(na.remove(hours_series))))
```

Plot the training data.

```{r}
p5data = fortify.zoo(hours_train)
n = length(hours_train)
p5 <- ggplot(p5data, aes(x = Index, y = hours_train)) +
  geom_line(color = "#410B13", linewidth = 0.1) +
  labs(
    title = "Monthly Average of Usual Hours Worked in Canada",
    subtitle = "Across all Industries from January 1987 to December 2020",
    y = "Monthly Mean Working Time (Hours)",
    x = "Year"
  ) + theme_bw() +
  theme(panel.grid.minor = element_line(
    color = "grey90",
    linetype = "dashed",
    linewidth = 0.5
  ))
print(p5)
```

## Part 2: Box-Jenkins Method

In this part, we select and fit a $\text{SARIMA}(p, d, q) \times (P, D, Q)_s$ model and make forecasts using the fitted model.

### Part A: Differencing

Difference the training set time series at lag 1. Let $\{X_{t}\}_{t \in \mathbb{Z}}$ be our original series. We define below $Y_t = \nabla^{1}X_t = X_t - X_{t-1}$
```{r}
hours_train_diff = diff(hours_train, lag = 1, differences = 1)
```
Plot the new time series and its correlogram. 

\underline{New Series}
```{r}
p6data = fortify.zoo(hours_train_diff)
p6 <- ggplot(p6data, aes(x = Index, y = hours_train_diff)) +
  geom_line(color = "#BA1F33", linewidth = 0.1) +
  labs(
    title = "Differenced Series of Monthly Average Hours Worked in Canada",
    subtitle = "Across all Industries from January 1987 to December 2020",
    y = "Monthly Mean Working Time (Hours)",
    x = "Year"
  ) + theme_bw() +
  theme(panel.grid.minor = element_line(
    color = "grey90",
    linetype = "dashed",
    linewidth = 0.5
  ))
print(p6)
```

\underline{Correlogram}

```{r}
p7data = data.frame(
  h = 0:36,
  rh = acf(hours_train_diff, plot = FALSE, lag.max = 36)$acf
)
p7 <- ggplot(p7data, aes(x = h, y = rh)) +
  geom_hline(yintercept = 2/sqrt(n),
             linetype = "dashed",
             col = "#421820") +
  geom_hline(yintercept = -2/sqrt(n),
             linetype = "dashed",
             col = "#421820") +
  ylim(-0.6, 1) +
  geom_segment(aes(xend = h, yend = 0),
               color = "#CD5D67",
               linewidth = 1) +
  geom_hline(yintercept = 0,
             linetype = "dashed",
             color = "darkgray") +
  labs(x = "Lag",  y = "ACF Values",
       title = "Correlogram of Differenced Series of Monthly Average Work Hours",
       subtitle = "Using Training Data for Canada, Jan. 1987 to Dec. 2020") +
    
  theme_bw()
print(p7)
```

\underline{Comment on what you observe.}

While the single-iteration ($d = 1$) of sequential differencing at lag 1 appears to have helped to mitigate the trend in the original training data, as seen in the plot of the differenced training time series, there still appears to be a notable seasonal component in the data. We know that this is likely the case due to the sinusoidal component in the correlogram, which is not really decreasing as a function of lag as we would like to see. This periodicity is indicative of a lingering seasonal component that we still need to difference out or remove via other methods.


### Part B: Seasonal Differencing

Apply seasonal differencing to remove seasonal variation. Since we have monthly data, $s = 12$ seems sensible.

```{r}
hours_train_diff_s = diff(hours_train_diff, lag = 12, differences = 1)
```

Plot the resulting differenced time series along with its sample acf and pacf. 
pure AR only for PACF
\underline{Series}
```{r}
p8data = fortify.zoo(hours_train_diff_s)
p8 <- ggplot(p8data, aes(x = Index, y = hours_train_diff_s)) +
  geom_line(color = "#9e0059", linewidth = 0.1) +
  labs(
    title = "Sequential and Seasonal [12] Difference of Monthly Average Hours Worked",
    subtitle = "In Canada, Across all Industries from January 1987 to December 2020",
    y = "Monthly Mean Working Time (Hours)",
    x = "Year"
  ) + theme_bw() +
  theme(panel.grid.minor = element_line(
    color = "grey90",
    linetype = "dashed",
    linewidth = 0.5
  ))
print(p8)
```

\underline{Sample ACF}
```{r}
p9data = data.frame(
  h = 0:80,
  rh = acf(hours_train_diff_s, plot = FALSE, lag.max = 80)$acf
)
p9 <- ggplot(p9data, aes(x = h, y = rh)) +
  geom_hline(yintercept = 2/sqrt(n),
             linetype = "dashed",
             col = "#240046") +
  geom_hline(yintercept = -2/sqrt(n),
             linetype = "dashed",
             col = "#240046") +
  ylim(-0.6, 1) +
  geom_segment(aes(xend = h, yend = 0),
               color = "#9c89b8",
               linewidth = 1) +
  geom_hline(yintercept = 0,
             linetype = "dashed",
             color = "darkgray") +
  labs(x = "Lag",  y = "ACF Values",
       title = "Correlogram of Sequential and Seasonal [12] Differences",
       subtitle = "Monthly Avg. Work Hours, Canada, Jan. 1987 to Dec. 2020") +
  theme_bw()
print(p9)
```
\underline{Sample PACF}

```{r}
p10data = data.frame(
  h = 1:80,
  rh = pacf(hours_train_diff_s, plot = FALSE, lag.max = 80)$acf
)
p10 <- ggplot(p10data, aes(x = h, y = rh)) +
  geom_hline(yintercept = 2/sqrt(n),
             linetype = "dashed",
             col = "#aa767c") +
  geom_hline(yintercept = -2/sqrt(n),
             linetype = "dashed",
             col = "#aa767c") +
  ylim(-0.5, 0.4) +
  geom_segment(aes(xend = h, yend = 0),
               color = "#63474d",
               linewidth = 1) +
  geom_hline(yintercept = 0,
             linetype = "dashed",
             color = "darkgray") +
  labs(x = "Lag",  y = "ACF Values",
       title = "Sample PACF of Sequential and Seasonal [12] Differences",
       subtitle = "Monthly Avg. Work Hours, Canada, Jan. 1987 to Dec. 2020") +
  theme_bw()
print(p10)
```

Comment on what you observe:

The seasonal differencing paired with the sequential differencing seems to have done a good job in making the series stationary. We see from the plotted data as well as the correlogram that there isn't evidence of a significant season or trend. Further, the visual complexity of both the ACF and the PACF hints at the fact that there may be a blended model (i.e. a mixture of AR and MA) underlying this process. We will discuss in the next sections exaclty what the parameters of the model equation are in this case. 


### Part C: Difference Parameters.
Based on the results of Part II (a) and (b), specify the values of $d$, $D$, and $s$.

From Part II (a), we applied one sequential difference using `diff`, so we know $d = 1$. Then, in Part II (b), we applied one seasonal difference using `lag = 12`, hence $D = 1$ with $s = 12$.


# PACF ONLY FOR PURE AR 
# TAKE SOME MA MODEL
### Part D: SARIMA Parameters

Based on the plots in Part II (b), suggest possible values of  justifying your choices.

*NOTE*: It is very difficult to accurately determine these coefficients precisely, but I will justify my decisions to the best of my ability.


Firstly, let's discuss the presence of an MA component. We henceforth assume stationarity.

In the ACF, we see a fairly large $r_h$ value at $h = 12$, that significantly cuts off for $h > 12$. Unlike a pure MA plot where we would see an expect a very sharp cutoff, in a blended model there will still be other components at work after the MA component. 

All of the lags after $h = 12$ for $h \in [0, 36] \subset \mathbb{Z}$ are noteably smaller, as the Sample ACF shows.

This would imply that either we have a sequential MA with $q = 12$ or a seasonal MA with $Q = 1$. By the prinicple of parsimony, we prefer the latter. Also, this conclusion would imply that it is unlikely that there is a $p$ component.

To explain, we recall the right-hand side of a SARIMA model, assuming $Q=1$.
$$
\begin{aligned}
\text{RHS } &= \Theta(B^s)\theta(B)Z_t \\
\text{RHS } &= (1 + \tilde{\beta}B^{12})\theta(B)Z_t \\
\text{RHS } &= \theta(B)Z_t + \theta(B)\tilde{\beta}Z_{t - 12} \\
\end{aligned}
$$
This would help explain the decently large "spike" at lag $12$ (also perhaps suggesting that $\tilde\beta < 0$.)

However, is there a sequential component $q > 0$? By the above, under the assumption that $Q = 1$, if $\theta(B)$ was of an order greater than zero, there would be additional components in the right-hand side that we'd expect to see reflected in the ACF. 

The most likely visually is that $q = 1$. An explanation of why is facilitated by the equation below: 
$$
\begin{aligned}
\text{RHS } &= (1 + \beta B)Z_t + (1 + \beta B)\tilde{\beta}Z_{t - 12} \\
\text{RHS } &= Z_t + \beta Z_{t-1} +\tilde{\beta}Z_{t - 12} + \beta\tilde{\beta}Z_{t - 13}  \\
\end{aligned}
$$
In other words, from our understanding of MA models, we would see large in magnitude $r_1$ and (slightly less large) $r_{13}$. In the sample ACF, with a spike at lag 1 less than $-2/\sqrt{n}$ and a shorter spike at lag 13, slightly above $2/\sqrt{n}$, however, it is difficult to discern whether or not these are caused by the AR components of the model. 

I am more inclined to favour the AR components causing these observed $r_h$ values than additional MA components due to the fact that there is a large autocorrelation at lag $11$, which isn't described by this sort of MA formulation. Notably, the fact that visually $|r_{11}| \approx |r_1|$ lends informal credence to the hypothesis that it is AR components rather than MA causing the $r_1$ and $r_{13}$ observations. If there were a $q>0$, I would expect it to have a notable magnitude outside of the general AR pattern. So, while it's theoretically plausible that $q>0$, by this reasoning we settle onto $q = 0$ in favour of a more robust AR components.


Now let's consider whether or not there are AR components to the series. Visually, the slowly-decaying oscillating pattern is indicative of one or more AR components. Further, since the lag doesn't cut off to white noise immediately after the MA components (we see it sometimes creep above the $\pm 2 / \sqrt{n}$) this tells us there is very likely an AR component of some variety. 


To figure out exactly what order these components are, we utilize the PACF, recalling the assumption of stationarity. 

However, we first must recall the left-hand side of the SARIMA equation, so we have a better understanding of the PACF Values
$$
\begin{aligned}
\text{LHS } &= \Psi(B^s)\psi(B)W_t \\
\text{LHS } &= \Psi(B^s)\psi(B)(\nabla^{12}_1 \nabla^1X_t) \\
\text{LHS } &= \Psi(B^s)\psi(B)(X_t-X_{t-12} - X_{t-1} + X_{t-13}) \\
\end{aligned}
$$
So, it's a bit more difficult to directly examine the PACF for the largest value of $\hat\alpha_{kk}$ such that $\forall i > k, |\hat\alpha_{ii}| < {2}\sqrt{n}$ as we normally would, since it isn't direct to discern which coefficient in the AR decomposition it actually aligns to in the overall model (i.e. in the full seasonal/sequential differenced with MA.)

Then we find a logical combination of $p$ and $P$, we can informally examine the PACF for what seems "sensible." What immediately jumps out to me is the presence of a large $\hat\alpha_{kk}$ at $k = 12$. This is indicative of a seasonal AR component, i.e. $P \geq 1$. Further, there is an immediate jump at $\hat\alpha_{11}$ as well as other "spikes" across the PACF at $\hat\alpha_{11, 11},  \hat\alpha_{23, 23}$, etc. Since these values $\{1, 11, 23, \dots \}$ are not evenly divisiple by $12$, we know that there must exist one or more sequential AR components, i.e. $p \geq 1$. For the sake of parsimony, my initial suggestion would be to choose $P = p = 1$ as a starting point. 


Due to the complexity inherent in this particular mixed models, it is difficult to visually determine values of $p$, $P$, $q$, and $Q$, however, from the arguments above, $\{p, P, q, Q \} = \{0, 1, 1, 1\}$ seems plausible.

<!-- We extract the value of $k$ below: -->
<!-- ```{r} -->
<!-- tail(p10data$h[abs(p10data$rh) >= 2/sqrt(n)], 1) -->
<!-- 41/12 -->
<!-- ``` -->
<!-- This value of $k = 54$ supports the observed PACF. However, since we already have a difference of $13$ from $W_t$, we consider $\kappa = k-13 = 41$ as our "target" for combination of AR components. -->


<!-- Moreover, since $41 \not \equiv 0\text{mod}(12)$, recalling $s = 12$, it suggests the presence of both sequential AR $p$ and seasonal AR $P$. For the simplest possible combination of coefficients, we first consider the maximum number of seasonal AR components we can add. -->

<!-- Mathematically, this is found as: -->
<!-- $$ -->
<!-- P =  \lfloor \kappa \div s\rfloor  =  \lfloor41 \div 12\rfloor =  3 -->
<!-- $$ -->
<!-- So, this suggests that there are $P = 4$ seasonal AR components. A hypothesized number of sequential AR components can be found directly as: -->
<!-- $$ -->
<!-- p = \kappa - P\cdot s = 41 -  -->
<!-- $$ -->


```{r}
# n = length(hours_train_diff_s)
# # (p,d,q): the AR order, the degree of differencing, and the MA order.
# p9data$rh[p9data$h]
# all_lags = acf(hours_train_diff_s, plot = FALSE, lag.max = n)$acf
# twelves = (all_lags[seq(from = 0, to = n, by = 1) %% 11 == 0])
# 
# plot(0:(length(twelves)-1), twelves, type="n",
#      xlab="Index", ylab="Value", main="Lags at Step 12")
# segments(x0=0:(length(twelves)-1), y0=0, x1=0:(length(twelves)-1), y1=twelves)
# abline(h = 0)
```
<!-- # acf(hours_train_diff_s, lag.max = 36) # MA at 12, Q = 1, MA at 1, q = 1  -->
<!-- #pacf(hours_train_diff_s, lag.max = 96) # PACF doesn't really show a  -->
<!-- #test = pacf(hours_train_diff_s, lag.max = 96, plot = F)$acf # P = 12 perhaps, P = 1 -->
<!-- # which( abs(test) > 2/sqrt(n) ) -->

### Part E: Iterative AIC

Now use the Akaike’s Information Criterion (AIC) to select the model based on the training dataset in Part I. Fix the values of p and P as your suggestions in Part II (d),


Consider $q \in [0, 5]$ and $Q \in [0, 5]$.

Select the values of q and Q according to the AIC values. 

Fit the model you choose and print the values of the estimated parameters along with the AIC value for the model.


First, we declare our fixed $p$, $P$, $s$, $d$ and $D$ values, then iterate through the different 25 models that can be developed and report the AIC for each. 

```{r, all_AIC}
p = 0; P = 0; s = 12; d = 1; D = 1
# declare empty matrix
results = matrix(0, nrow = 6, ncol = 6)
# with legible names
rownames(results) = c("q=0", "q=1", "q=2","q=3","q=4","q=5")
colnames(results) = c("Q=0", "Q=1", "Q=2","Q=3","Q=4","Q=5")
# iterate through all candidates
for (q in 0:5){
  for (Q in 0:5){
    results[q+1, Q+1] = arima(hours_train,
                          order = c(p, d, q),
                          seasonal = c(P, D, Q))$aic
  }
}

```

The AIC results are summarized in the table below.
```{r}
kable(round(results, 3), caption = "AIC Values for Varying (q, Q)")
```

According to this table, the best model corresponds to:
```{r}
which(results == min(results), arr.ind = TRUE)

# arima(hours_train, order = c(1, 1, 0), seasonal = c(1, 1, 0))$aic
# arima(hours_train, order = c(1, 1, 0), seasonal = c(1, 1, 1))$aic
min(results)
```
Which corresponds to $q = 4$ and $Q = 5$. It has an AIC of $-627.61$

Hence, we will use a model of the form:
$$
\text{SARIMA}({\underset{p}{0}}, {\underset{d}{1}}, {\underset{q}{4}}) \times({\underset{P}{0}}, {\underset{D}{1}}, {\underset{Q}{5}})_{{\underset{s}{12}}}
$$
The full model is given below:
```{r}
best_fit = arima(hours_train, order = c(0, 1, 1), seasonal = c(1, 1, 5))
#?auto.arima(hours_train)
```

```{r}
# Series: hours_train 
# ARIMA(1,0,0)(0,1,4)[12] with drift 
# auto.arima(hours_train, trace = F, ic = "aic",
#            start.p = 1, start.P = 1, stepwise = F, 
#            max.Q = 5, max.q = 5, max.d = 1, max.D = 1)

# predict(best_fit, prediction.interval = T, level = 0.95)
```

```{r}
# best_fit$coef
auto.arima(hours_train, D = 1, d = 1, stepwise = F,
           ic = "aic")
```
```{r}
# auto.arima(hours_train, 
#            d = 1, D = 1,
#            max.p = 5, max.P = 5, 
#            max.q = 5, max.Q = 5, 
#            start.P = 3, start.p = 5,
#            start.q = 0, start.Q = 0,
#            stepwise = FALSE, approximation = TRUE)
```

<!-- ```{r} -->
<!-- # TODO -->
<!-- ``` -->

<!-- ```{r} -->
<!-- auto.arima(hours_train) -->
<!-- ``` -->

