
---
title: "Stat 443: Time Series and Forecasting"
subtitle: "Assignment 2: Time Series Models"
author: Caden Hewlett
header-includes:
    - \usepackage{upgreek}
    - \usepackage{bbm}
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(zoo)
library(tseries)
library(ggplot2)
library(gridExtra)
library(reshape2)
library(forecast)
library(ggfortify)
library(grid)
library(gt)
library(gtExtras)
# library(mgcv)
```

# Questions

## 1. Stationary Second-Order AR Process

Consider the stationary second-order AR process:

$$
X_t = \frac{7}{10}X_{t-1} - \frac{1}{10}X_{t-2} + Z_t
$$

Where $\{Z_t\}_{t \in \mathbb{Z}} \sim \text{WN}(0, \sigma^2)$.



### a) Yule-Walker Equations

Derive the Yule-Walker equations and find the autocorrelation functions of ${X_t}_{t \in \mathbb{Z}}$.

Let us begin by deriving the Yule-Walker Equations.


**Step 1**: The first step is to multiply each side of the equation by $X_{t-i}$ for $i \in [1,p]$.
$$
\begin{aligned}
X_tX_{t-1} &= \frac{7}{10}X_{t-1}X_{t-1} - \frac{1}{10}X_{t-1}X_{t-2} + Z_tX_{t-1} \\
X_tX_{t-2} &= \frac{7}{10}X_{t-1}X_{t-2} - \frac{1}{10}X_{t-2}X_{t-2} + Z_tX_{t-2}
\end{aligned}
$$
**Step 2**: Then we take the expectation of these equations, using linearity of expectation directly. 
$$
\begin{aligned}
\mathbb{E}(X_tX_{t-1}) &= \frac{7}{10}\mathbb{E}(X_{t-1}X_{t-1}) - \frac{1}{10}\mathbb{E}(X_{t-1}X_{t-2}) + \mathbb{E}(Z_tX_{t-1}) \\
\mathbb{E}(X_tX_{t-2}) &= \frac{7}{10}\mathbb{E}(X_{t-1}X_{t-2}) - \frac{1}{10}\mathbb{E}(X_{t-2}X_{t-2}) + \mathbb{E}(Z_tX_{t-2}) \\
\text{This simplifies} &\text{ to...}\\
\gamma(1) &= \frac{7}{10}\gamma(0) - \frac{1}{10}\gamma(1)  \\
\gamma(2) &= \frac{7}{10}\gamma(1) - \frac{1}{10}\gamma(0) \\
\end{aligned}
$$
**Step 3**: Divide everything by $\gamma(0)$ to get correlation values.
$$
\begin{aligned}
\rho(1) &= \frac{7}{10} - \frac{1}{10}\rho(1)  \\
\rho(2) &= \frac{7}{10}\rho(1) - \frac{1}{10} \\
\end{aligned}
$$
Then, we can solve directly to find:
$$\rho(1) = \frac{7}{11}, \text{ and } \rho(2) = \frac{19}{55}$$
However, we're not done! We still need to find the general autocorrelation function for all $h$ values, i.e. $\rho(h)$.

To do this, we must reconsider the original AR model, which I'll write as:
$$
\begin{aligned}
\alpha_0X_t = \alpha_1X_{t-1} + \alpha_2X_{t-2} + Z_t \\
\text{ Where } \alpha_0 = 1, \alpha_1 = \frac{7}{10} \text{ and } \alpha_2 = - \frac{1}{10}
\end{aligned}
$$
With this definition of $\alpha_i$ for $i \in [0,2]$ we can write the polynomial:
$$
D^p - \alpha_1D^{p-1} - \alpha_2 D^{p-2} = D^2 - \frac{7}{10}D + \frac{1}{10}
$$ 
Then, we solve the roots of this polynomial:
$$
\begin{aligned}
0 &= D^2 - \frac{7}{10}D + \frac{1}{10} \\
0 &= 10D^2 - {7}D + {1} \\
0 &= (5D - 1)(2D -1) \\
&\text{Hence, } d_1 = \frac{1}{5} \text{ and } d_2 = \frac{1}{2}
\end{aligned}
$$
We then substitue these into the General Equation for $\rho(h)$.
$$
\rho(h) =  A_1 d_1^{|h|} +  A_2 d_2^{|h|} \rightarrow \rho(h) = A_1\Big(\frac{1}{5}\Big)^{|h|} +  A_2\Big(\frac{1}{2}\Big)^{|h|}
$$
We'll use our known solutions of $\rho(0) = 1$ and $\rho(1) = 7 / 11$ to find $A_1$ and $A_2$.
$$
\begin{aligned}
\rho(0) &= A_1 + A_2 = 1 \text{ hence } A_1 = 1-A_2\\
\rho(1) &= \frac{A_1}{5} + \frac{A_2}{2} = \frac{7}{11} \\
\end{aligned}
$$
Substituting $A_1 = 1-A_2$ into the second equation, 

$$
\begin{aligned}
 \frac{1}{5}(1 - A_2) + \frac{1}{2}(A_2) &= \frac{7}{11} \\
\frac{3}{10}A_2 &= \frac{24}{55} \\
\text{ Hence } A_2 &= \frac{16}{11}, \text{ and } A_1 = -\frac{5}{11}
\end{aligned}
$$
Thus, we can write the general acf function at lag $h$ as:
$$
\rho(h) =  \frac{16}{11}\Big(\frac{1}{2}\Big)^{|h|}-\frac{5}{11}\Big(\frac{1}{5}\Big)^{|h|} 
$$

### b) Simulation and Plot

Assume $\sigma^2 = 1$. Using `set.seed(443)` to set the simulation seed and `arima.sim()` function simulate 1000 observations from the AR(2) process defined above. Plot its sample ACF for the first 15 lags along with the theoretical ACF obtained in part (a). Compare the sample and theoretical ACFs.

```{r}
my_acf = function(h){
  (16/11)*((1/2)^h) - (5/11)*((1/5)^h)
}
theo = sapply(0:15, my_acf)
# R code block for simulation and plotting
set.seed(443)
# Further simulation and plotting code goes here
sigma_2 = 1
sim = arima.sim(n = 1000, model = list(ar = c(7/10, -1/10)), sd = sigma_2)

# lines(sapply(0:15, my_acf), col = 'red')
## resids

```

```{r}
set.seed(443)
theo = sapply(0:15, my_acf)
resids = sapply(seq(from = 500, to = 11000, by = 100), 
                function(n)
                  {
                  sim = arima.sim(n = n, model = list(ar = c(7/10, -1/10)), sd = sigma_2)
                  sample_acf = acf(sim, lag.max = 15, plot = FALSE)
                  rss = sum((sample_acf$acf - theo)^2)
                  rss
                })

plot(y = resids, x = seq(from = 500, to = 11000, by = 100), ylab = "RSS", xlab = "Sample Size")
points(y = resids[6], x = 1000, col = 'red', pch = 12)
```


## 2. ARMA(2,1) Process

Consider the ARMA(2,1) process:

$$
X_t = -0.5X_{t-2} + Z_t + 0.5Z_{t-1}
$$
Where $\{Z_t\}_{t \in \mathbb{Z}} \sim \text{WN}(0, \sigma^2)$.

### a) Stationarity and Invertibility

Check whether the process is stationary and invertible. Justify your answers.

We will begin by converting $\{X_t\}$ into a function of the characteristic polynomials $\varphi(B)$ and $\theta(B)$. We let $B$ denote the backshift operator. 

By rearrangement, 
\begin{align*}
X_t &= -0.5X_{t-2} + Z_t + 0.5Z_{t-1} \\
X_t + 0.5X_{t-2}  &=  Z_t + 0.5Z_{t-1} \\
X_t(B^0 + 0.5B^2)  &=  Z_t(B^0 + 0.5B^1) \\
X_t(1+ 0.5B^2)  &=  Z_t(1+ 0.5B) 
\end{align*}

Therefore, we can write our ARMA(2,1) process as:
\begin{align*}
\text{ARMA}(2, 1) : \varphi(B)X_t = \theta(B)Z_t\\
\text{Where }  \begin{cases}\varphi(B) =  1+0.5B^2 \\
\theta(B) = 1 + 0.5B \end{cases}
\end{align*}

Now, we have everything to need to verify invertibility and stationarity. 

**Invertibility**: We know that $\{X_t\}$ is invertible.


*Proof:* Let $\tilde{\theta}_i \in \mathbb{C}$ be the $i$-th root of $\theta(B)$. In order for the ARMA process to be invertible, it must hold that all roots of $\theta(B)$ are greater than the roots of unity. In other words, we wish to show that: 
$$\forall i \in[1,q],\, \|\tilde{\theta}_i \| > 1$$
Hence, we must find the roots of $\theta(B)$ and their magnitude. By the Fundamental Theorem of Algebra, we know that there will exist exactly one root $\tilde{\theta}_1 \in \mathbb{C}$, as $\theta(B)$ has order 1.

Directly, we see:
\begin{align*}
1 + 0.5B &= 0 \\
2 + B &= 0 \\
B &= -2 
\end{align*}
$$\text{Hence, } \tilde{\theta}_1 = -2 , \text{ and } \|\tilde{\theta}_1\| = 2 > 1\; \therefore \{X_t\} \text{ is invertible.} \;\;\square$$

Thus, we have proved $\{X_t\}$ is invertible, as required.



**Stationarity** We know that $\{X_t\}$ is stationary.


*Proof:* Let $\tilde{\varphi}_j \in \mathbb{C}$ be the $j$-th root of $\varphi(B)$. In order for the ARMA process to be stationary, it must hold that all roots of $\varphi(B)$ are greater than the roots of unity. In other words, we wish to show that: 
$$\forall j \in[1,p],\, \|\tilde{\varphi}_i \| > 1$$
Hence, we must find the roots of $\varphi(B)$ and their magnitude. By the Fundamental Theorem of Algebra, we know that there will exist exactly two roots $\{\tilde{\varphi}_1, \tilde{\varphi}_2\} \in \mathbb{C}$, as $\varphi(B)$ has order 2.

We can find them directly.
\begin{align*}
1 + 0.5B^2 &= 0 \\
2 + B^2 &= 0 \\
B &= \pm\sqrt{-2}
\end{align*}

Hence, 
$$\{\tilde{\varphi}_1, \tilde{\varphi}_2\} = \{i\sqrt{2}, -i\sqrt{2}\}
$$
Further, we know that 
$$\|\tilde{\varphi}_1\| = \| \tilde{\varphi}_2 \| = \sqrt{2} > 1, \; \therefore \{X_t\} \text{ is stationary.} \;\;\square\\$$
Thus, we have proved $\{X_t\}$ is stationary, as required.

In conclusion, we have shown that the ARMA(2, 1) process is both stationary and invertible.


### b) Pure AR Process: 

<!-- Pure MA? -->

Write the above ARMA(2,1) process as a pure AR process.


In order to write the model as a pure AR process, we will express it in the following form:
$$
Z_t = \bigg(\frac{\varphi(B)}{\theta(B)}\bigg)X_t = \mathcal{\uppi}(B)X_t
$$

Where $\theta(B)$ and $\varphi(B)$ are as defined in the previous question. Let us find an expression for ${\uppi}(B)$:
$$
\mathcal{\uppi}(B) = \frac{\varphi(B)}{\theta(B)} = \dfrac{(1+0.5B^2)}{(1 + 0.5B)} = \dfrac{(1+0.5B^2)}{\big(1 - (-0.5B)\big)} = (1+0.5B^2)\sum_{n=0}^{\infty}(-0.5B)^n
$$
Where the last expression in the equality follows from the definition of a geometric series, under the assumption that $|-0.5B| < 1$.
We expand this process, letting $\delta = 0.5$ for readability.
$$
\begin{aligned}
{\uppi}(B) &= (1+ \delta B^2)\sum_{n=1}^{\infty}(-\delta B)^n \\
{\uppi}(B) &= (1+ \delta B^2)\Big[1 + (-\delta B) + (-\delta B)^2 + (-\delta B)^3 + \dots \Big] \\
{\uppi}(B) &= \Big[ 1 + (-\delta B) + \big(\delta B^2 +(- \delta B)^2  \big) + \big((\delta B^2)(-\delta B) +(- \delta B)^3\big) + \dots\Big]
\end{aligned}
$$
Now, we can begin to attempt to investigate each $\pi_i \in \uppi(B)$ where $i$ indicates the power of $B$ in the expression.
$$
\begin{aligned}
\pi_0 &= 1 \\
\pi_1 &=  (-\delta B)\\
\pi_2 &= \delta B^2 + (- \delta B)^2 \\
\pi_3 &= (\delta B^2)(-\delta B) +(- \delta B)^3 \\
\pi_4 &= (\delta B^2)(-\delta B)^2 +(- \delta B)^4 \\
\vdots 
\end{aligned}
$$
Investigating these terms, we begin to notice a pattern:
$$
\begin{aligned}
\pi_0 &=  (0 + (-\delta)^0)B^0 \\
\pi_1 &= (0 + (-\delta)^1)B^1  \\
\pi_2 &= (\delta + (- \delta)^2)B^2 \\
\pi_3 &= (\delta(-\delta) + (-\delta)^3)B^3 \\
\pi_4 &= ( \delta(-\delta )^2 +(- \delta )^4) B^4 \\
\vdots  \\
\pi_j  &= (\delta (-\delta)^{j-2} + (-\delta)^j)B^j
\end{aligned}
$$
With this in mind, we can denote the following term to define all elements of $\uppi(B)$ for $j \in \mathbb{N}$ where, still using $\delta = 0.5$, each $\pi_j$ term is defined by the rule stated (using indicators to account for the fact that $\pi_0$ and $\pi_1$ follow a slightly different general pattern.)
$$
\uppi(B) = 1-\sum_{j=0}^{\infty} B^j\pi_j, \text{ where } \{\pi_j\} =  \big(\mathbbm{1}[j \geq 2]\delta(-\delta)^{j - 2}\big) + (- \delta)^j
$$
Alternatively, we can write $\{\pi_j\}$ by uniquely declaring the first two terms, and re-substituting $\delta = 0.5$. Then, we wouldn't have to write it with indicators.
$$
\{\pi_j\} := \big\{\pi_0 =1, \pi_1 = -0.5, \forall j \geq2, \pi_j = 0.5(-0.5)^{j-2} + (-0.5)^j\big\}
$$
Now that we have a clear definition for $\uppi(B)$, we can conclude by writing:

$$
 \uppi(B)X_t = Z_t
$$
Thus, we've converted the ARMA process to a pure infinite-order AR process with a new characteristic polynomial $\uppi(B)$ as required.


<!-- We can hence succintly represent the general form of $\uppi(B)$ as follows: -->
<!-- $$\uppi(B) = \Big\{  \pi_j \mid \pi_0 =1, \delta = 0.5,\text{ and } \forall j \in \mathbb{N}, \pi_j = (\delta B^2 -\delta B)(-\delta B) ^{j-1} \Big\}$$ -->
<!-- Or, alternatively, as the function: -->
<!-- $$ -->

<!-- $$ -->

### c) Pure MA Process 

Write the above ARMA(2,1) process as a pure MA process.

In order to write the model as a pure MA process, we will express it in the following form:
$$
X_t = \bigg(\frac{\theta(B)}{\varphi(B)}\bigg)Z_t = \Psi(B)Z_t
$$

Where $\theta(B)$ and $\varphi(B)$ are as defined in the previous question.Let us find an expression for $\Psi(B)$:
$$
{\Psi}(B) = \frac{\theta(B)}{\varphi(B)} = \dfrac{(1 + 0.5B)}{(1+0.5B^2)} = \dfrac{(1 + 0.5B)}{\big(1 - (-0.5B^2)\big)} = (1+0.5B)\sum_{n=0}^{\infty}(-0.5B^2)^n
$$
Where the last expression in the equality follows from the definition of a geometric series, under the assumption that $|-0.5B^2| < 1$.
Let's expand this product's infinite sum a bit, again letting $\delta = 0.5$.
$$
\begin{aligned}
\Psi(B) &=(1+\delta B)\sum_{n=0}^{\infty}(-\delta B^2)^n \\
\Psi(B) &=(1+\delta B)\big(1 + (-\delta B^2 )+ \delta^2 B^4 + (- \delta^3 B^6) + \dots ) \\
\Psi(B) &= \big( 1 + \delta B - \delta B^2 - \delta ^2 B^3 + \delta^2 B^4 -\delta^3 B^5 - \delta^3B^6 + \dots )
\end{aligned}
$$
Investigating these in terms of the powers of $B$, we begin to see a pattern. It's slightly more complicated than the last one, because of the squared term in the infinite sum.
$$
\begin{aligned}
\psi_0 &= 1 = (-1)^0(-\delta)^0B^0\\ 
\psi_1 &= \delta B = (-1)^1(-\delta)^1B^1\\
\psi_2 &= -\delta B^2 = (-1)^2(-\delta)^1B^2\\
\psi_3 &= -\delta^2 B^3 = (-1)^3(-\delta)^2B^3 \\
\psi_4 &= \delta^2 B^4 = (-1)^4(-\delta)^2B^4 \\
\psi_5 &= \delta^3 B^5 = (-1)^5(-\delta)^3B^5 \\
\psi_6 &= -\delta^3 B^6 = (-1)^6(-\delta)^3B^6 \\
\vdots \\
\psi_j &= \overset{\text{?}}{\dots} = (-1)^j(-\delta)^{\lceil j / 2\rceil} B^j
\end{aligned}
$$
The series for $\psi_i$ seems to have a "lagging" component in the $\delta$ terms. It could even seem as if there's a different series for odd and even indices. However, we can encapsulate the series with the ceiling function, as shown above.

We could alternatively express the series as a piecewise function, if we don't wish to use the ceiling function.

$$
\psi_j =  \begin{cases}(-\delta)^{j/2} B^j, & j \equiv 0\text{mod}(2) \\
-(-\delta)^{(j+1)/2}B^j, & j \equiv 1 \text{mod}(2)\end{cases} 
$$
In either case, we can write $\Psi(B)$ in its entirety. Letting $\delta = 0.5$, we write:

$$
\Psi(B) = \sum_{i = 0}^{\infty} \psi_i B^i, \text{ where } \{\psi_j\} = (-1)^{\,j}(-\delta)^{\lceil j / 2\rceil} B^{\,j}
$$
By this definition, we conclude that:

$$
X_t = \Psi(B)Z_t, \text{ for } \Psi(B) = \sum_{i = 0}^{\infty} \psi_i B^i
$$

### d) Autocorrelation Function

Find the ACF of $\{X_t\}_{t \in \mathbb{Z}}$.

For this question, we continue to use the nomenclature of the previous questions; namely, that $\delta = 0.5$. To begin, we will find the autocovariance function using the MA representation from **Part C**. 

We will use the piecewise definition. However, we make the following adjustments to the terminology:
$$
\psi_i = \begin{cases} \psi_i^{\text{E}}B^{\,i}, &i\text{ is even.} \\
\psi_i^{\text{O}}B^{\,i}, &i \text{ is odd}.\end{cases}
$$
Where, 
$$
\psi_i^{\text{E}} = (-\delta)^{i /2}, \text{ and } \psi_i^{\text{O}} = -(-\delta)^{(j+1)/2}
$$
We consider the model in the MA form, $X_t = \Psi(B)Z_t$ to find the covariance $\gamma(h) =\text{cov}(X_t, X_{t+h})$.
$$
\begin{aligned}
\text{cov}(X_t, X_{t+h}) &= \text{cov}\big(\Psi(B)Z_{t}, \Psi(B)Z_{t+h}\big) 
\end{aligned}
$$
Noting that $\mathbb{E}(X_t) = 0$, we can write $\text{cov}(X_t, X_{t+h}) = \mathbb{E}(X_t, X_{t+h})$:
$$
\begin{aligned}
\gamma(h) &= \mathbb{E}\bigg(\sum_{i=0}^{\infty}\psi_iZ_{t-i}, \sum_{j=0}^{\infty}\psi_jZ_{t-j+h}\bigg) \\
\gamma(h) &= \sum_{i=0}^{\infty}\sum_{j=0}^{\infty} \psi_i \psi_j\mathbb{E}\bigg(Z_{t-i}, Z_{t-j+h}\bigg) 
\end{aligned}
$$
Letting $j = i + h$, we can unify the summations:
$$
\begin{aligned}
\gamma(h) &= \sum_{i=0}^{\infty} \psi_i \psi_{i+h}\mathbb{E}\big(Z_{t-i}, Z_{t-i}\big) \\
\gamma(h) &= \sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_{i+h}
\end{aligned}
$$
Now, we need to evaluate the summation of these two $\psi$ terms. This is where we return to our Even/Odd representation from earlier. 
$$
\begin{aligned}
\gamma(h) &= \sigma^2 \sum_{i\text{ is even}}^{\infty} \psi_i \psi_{i+h} + \sigma^2 \sum_{j\text{ is odd}}^{\infty} \psi_j \psi_{j+h}
\end{aligned}
$$
However, there is an additional step to consider. We cannot simply factor in our $\psi_i^{\text{E}}$ and $\psi_i^{\text{O}}$ polynomials, because the \underline{parity of $h$} impacts the polynomial choice, too.

Hence, we have to consider the cases where $h$ is even and odd as well. If $h$ is even, then $i + h$ remains even and $j + h$ remains odd. Therefore, 
$$
\gamma(h \mid h \text{ is even}) = \sigma^2 \sum_{i\text{ is even}}^{\infty} \psi_i^{\text{E}} \psi_{i+h}^{\text{E}} + \sigma^2 \sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{O}}
$$
Similarly, if $h$ is odd, $i + h$ becomes odd and $j + h$ becomes even. Therefore,
$$
\gamma(h \mid h \text{ is odd}) = \sigma^2 \sum_{i\text{ is even}}^{\infty} \psi_i^{\text{E}} \psi_{i+h}^{\text{O}} + \sigma^2 \sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{E}}
$$
Hence, our equation for $\gamma(h)$ is as follows (moving $\sigma^2$  outside the cases for simplicity):
$$
\gamma(h) = \sigma^2\begin{cases} \sum_{i\text{ is even}}^{\infty} \psi_i^{\text{E}} \psi_{i+h}^{\text{E}} + \sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{O}}\,, &h\text{ is even.} \\
 \sum_{i\text{ is even}}^{\infty} \psi_i^{\text{E}} \psi_{i+h}^{\text{O}} +  \sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{E}}\,, &h \text{ is odd}.\end{cases}
$$
We evaluate each of these cases in turn.

**Case 1:** $h$ is even.

We'll evaluate the sum of sums:
$$
\gamma(h \mid h \text{ is even}) =\sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{E}} + \sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{O}}
$$
We start with evaluating the first term in the sum. It is crucial to remember that each $\psi$ is itself an infinite-order polynomial, so we must write them as:
$$
\begin{aligned}
\sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{E}} &= \sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \sum_{i\text{ is even}}^{\infty}\psi_{i+h}^{\text{E}} 
\end{aligned}
$$
Now, we evaluate recalling $\delta = 0.5$:
$$
\begin{aligned}
\sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{E}}&= \sum_{i\text{ is even}}^{\infty}(-\delta)^{i/2} \sum_{i\text{ is even}}^{\infty}(-\delta)^{i+h/2} \\
\sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{E}}&= \sum_{i\text{ is even}}^{\infty}(-\delta)^{i/2} \sum_{i\text{ is even}}^{\infty}(-\delta)^{h/2}(-\delta)^{i/2} \\
\sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{E}}&= (-\delta)^{h/2}\bigg(\sum_{\;i\text{ is even}}^{\infty}(-\delta)^{i/2} \bigg)^2\\
\sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{E}}&= (-\delta)^{h/2}\big(1 - \delta + \delta^2 - \delta^3 + \dots\big)^2\\
\sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{E}}&= (-\delta)^{h/2}\Big(\sum_{\ell = 0}^{\infty}(-\delta)^{\ell}\Big)^2 \\
\sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{E}}&= (-\delta)^{h/2}\Big(\frac{1}{1-(-0.5)}\Big)^2 \\
\sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{E}}&= (-\delta)^{h/2}\Big(\frac{2}{3}\Big)^2 \\
\sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{E}}&= \frac{4}{9}(-\delta)^{h/2}
\end{aligned}
$$
We henceforth use that $\sum_{\ell = 0}^{\infty}(-\delta)^{\ell} = 2/3$ without showing the expansion.

Let us continue with the second term.
$$
\begin{aligned}
\sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{O}} &= \sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \sum_{j\text{ is odd}}^{\infty}\psi_{j+h}^{\text{O}} \\
\sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{O}} &= \sum_{j\text{ is odd}}^{\infty} -(-\delta)^{(i + 1)/2}\sum_{j\text{ is odd}}^{\infty}-(-\delta)^{(i + 1 + h)/2} \\
\sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{O}} &= (-\delta)^{h/2}\bigg(\sum_{j\text{ is odd}}^{\infty} -(-\delta)^{(i + 1)/2}\bigg)^2 \\
\sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{O}} &= (-\delta)^{h/2}\big(-\delta + \delta^2 - \delta^3 + \delta^4 + \dots\big)^2 \\
\sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{O}} &= (-\delta)^{h/2}\Big(-1 + \sum_{\ell = 0}^{\infty}(-\delta)^{\ell}\Big)^2 \\
\sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{O}} &= (-\delta)^{h/2}\Big(-1 + \frac{2}{3}\Big)^2 \\
\sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{O}} &= \frac{1}{9}(-\delta)^{h/2}
\end{aligned}
$$
Hence, we can add these two togehter to get $\gamma(h)$ in the even case:
$$
\gamma(h \mid h \text{ is even}) =  \sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{E}} + \sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{O}} = \frac{4}{9}(-\delta)^{h/2}+\frac{1}{9}(-\delta)^{h/2} = {\frac{5}{9}(-0.5)^{h/2}}
$$
**Case 2:** $h$ is odd.

Now, we evaluate the second part of the piece-wise function:

$$
\gamma(h \mid h \text{ is odd}) = \sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{O}} +  \sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{E}}
$$
We'll begin with the first term in the sum.
$$
\begin{aligned}
\sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{O}} &= \sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{O}}  \\
\sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{O}} &= \sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \sum_{i\text{ is even}}^{\infty}\psi_{i+h}^{\text{O}} \\
\sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{O}} &= \sum_{i\text{ is even}}^{\infty}(-\delta^{i/2}) \sum_{i\text{ is even}}^{\infty}-(-\delta)^{(i+1+h)/2} \\
\sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{O}} &= \Big(\sum_{\ell = 0}^{\infty}-\delta^{-\ell}\Big) \sum_{i\text{ is even}}^{\infty}-(-\delta)^{(i+1+h)/2}  \\
\sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{O}} &= \frac{2}{3} \sum_{i\text{ is even}}^{\infty}-(-\delta)^{i/2}(-\delta)^{(1+h)/2} \\
\sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{O}} &= -\frac{2}{3}(-\delta)^{(1+h)/2} \sum_{i\text{ is even}}^{\infty}(-\delta)^{i/2} \\
\sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{O}} &= -\frac{4}{9}(-\delta)^{(1+h)/2} 
\end{aligned}
$$

Now, we move onto the final term.

$$
\begin{aligned}
\sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{E}} &= \sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}}\sum_{j\text{ is odd}}^{\infty}\psi_{j+h}^{\text{E}} \\
\sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{E}} &= \sum_{j\text{ is odd}}^{\infty} -(-\delta)^{(j+1)/2}\sum_{j\text{ is odd}}^{\infty}(-\delta)^{(j + h )/2} \\
\sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{E}} &= -\sum_{j\text{ is odd}}^{\infty} (-\delta)^{(j+1)/2}\sum_{j\text{ is odd}}^{\infty}(-\delta)^{(j -1+ h + 1 )/2} \\
\sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{E}} &= -\bigg(-1 +\sum_{\ell = 0}^{\infty}(-\delta)^{\ell}\bigg) \sum_{j\text{ is odd}}^{\infty}(-\delta)^{(j -1+ h + 1 )/2} \\
\sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{E}} &= -\bigg(-\frac{1}{3}\bigg) \sum_{j\text{ is odd}}^{\infty}(-\delta)^{(j -1+ h + 1 )/2}  \\
\sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{E}} &= \frac{1}{3} (-\delta)^{(h + 1)/2}\sum_{j\text{ is odd}}^{\infty}(-\delta)^{(j -1 )/2}\\
\sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{E}} &= \frac{1}{3} (-\delta)^{(h + 1)/2}\big(1 -\delta + \delta^2 - \delta^3 +\dots)\\
\sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{E}} &= \frac{1}{3} (-\delta)^{(h + 1)/2}\Big(\sum_{\ell = 0}^{\infty}(-\delta)^{\ell} \Big) \\
\sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{E}} &= \frac{1}{3} (-\delta)^{(h + 1)/2}\Big( \frac{2}{3}\Big) \\
\sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{E}} &= \frac{2}{9} (-\delta)^{(h + 1)/2} 
\end{aligned}
$$

Putting this together, we can derive an overall expression for the odd case:
$$
\gamma(h \mid h \text{ is odd}) = \sum_{i\text{ is even}}^{\infty}\psi_i^{\text{E}} \psi_{i+h}^{\text{O}} +  \sum_{j\text{ is odd}}^{\infty} \psi_j^{\text{O}} \psi_{j+h}^{\text{E}} = -\dfrac{4}{9}(-\delta)^{(h + 1)/2}  + \frac{2}{9} (-\delta)^{(h + 1)/2}  = -\dfrac{2}{9}(-0.5)^{(h + 1)/2} 
$$
Putting this all together, we can write the piece-wise expression for $\gamma(h)$.
$$
\gamma(h) = \sigma^2\begin{cases}{\dfrac{5}{9}(-0.5)^{h/2}}\, &h\text{ is even.} \\
-\dfrac{2}{9}(-0.5)^{(h + 1)/2}  &h \text{ is odd}.\end{cases}
$$
However, this is not yet the *acf*. To get the *acf*, we must divide $\gamma(h)$ by $\gamma(0)$. Noting that zero is even, we can write:
$$
\gamma(0) = \sigma^2(-0.5)^{0/2} = \dfrac{5\sigma^2}{9}
$$
Then, dividing the acvf by $\gamma(0)$, we arrive at the true acf for the ARMA process.
$$
\rho(h) = \begin{cases}{(-0.5)^{h/2}}\, &h\text{ is even.} \\
-\dfrac{2}{5}(-0.5)^{(h + 1)/2}  &h \text{ is odd}.\end{cases} 
$$
Thus ends the most ridiculous sub-question I have ever done. $\square$



## 3. AR(2) Process

Consider the AR(2) process given by:

$$
2aX_t + \frac{3}{6a}X_{t-1} - \frac{1}{6a}X_{t-2} = Z_t, \quad a \in \mathbb{R}
$$

### a) Stationarity Conditions

Under which conditions on constant $a$ is the process $\{X_t\}_{t \in \mathbb{Z}}$ stationary?

Let's adjust the expression in terms of the backshift operator $B$.

$$\Big(2aB^0 + \frac{3}{6a}B - \frac{1}{6a}B^2\Big)X_t = Z_t$$
From this, we can conclude that the characteristic polynomial $\varphi(B)$ can be written as:
$$
\varphi(B) = 2a + \frac{3}{6a}B - \frac{1}{6a}B^2
$$
Again, let $\tilde{\varphi}_j \in \mathbb{C}$ be the $j$-th root of $\varphi(B)$. In order for the ARM process to be stationary, it must hold that all roots of $\varphi(B)$ are greater than the roots of unity. In other words, we wish to show that: 
$$\forall j \in[1,p],\, \|\tilde{\varphi}_i \| > 1$$
By the Fundamental Theorem of Algebra, there are two roots $\{\tilde{\varphi}_1, \tilde{\varphi}_2\}$ that we can find for this characteristic polynomial. We should note from the beginning that in order for the polynomial to be well defined, $a \neq 0$. 

Let us solve:

- check roots of $(2a + (3B)/6A - (B^2)/6a)

### b) Autocorrelation Function

Assuming that $a$ satisfies conditions found in part (a), find the ACF of $\{X_t\}_{t \in \mathbb{Z}}$.

1.) yule walker

2.) find $d_i$

3.) find $A_i$

4.) done

## 4. SARIMA Model

Show that $\text{SARIMA}(\underset{p}{2}, \underset{d}{1}, \underset{q}{1}) \times (\underset{P}{0}, \underset{D}{1}, \underset{Q}{1})_{\underset{s}{12}}$ can be written as an ARMA(p,q) process and specify values of $p$ and $q$.


Let us begin by finding the $p$-value for the AR process. 

For a $\text{SARIMA}(\underset{p}{2}, \underset{d}{1}, \underset{q}{1}) \times (\underset{P}{0}, \underset{D}{1}, \underset{Q}{1})_{\underset{s}{12}}$

$$
\text{LHS} = \varphi(B)\Phi(B^s)W_t, \text{ where } W_t = \nabla^d \nabla^D_s X_t 
$$
We wish to represent it in ARMA form, which has a single polynomial on each side. In other words, let the following be our "desired" ARMA process:

$$
{\varphi}^{\oplus}(B)X_t = {\theta}^{\oplus}(B)Z_t
$$
Where ${\varphi}^{\oplus}(B)$ and ${\theta}^{\oplus}(B)$ are the characteristic polynomials of the ARMA model. Let's begin find $p$, the order of ${\theta}^{\oplus}(B)$, by expanding the $\text{LHS}$ of the SARIMA model.

In order to do this, we must first expand $W_t$. 
$$
\begin{aligned}
W_t &= \nabla^d \nabla^D_s X_t \\
W_t &= \nabla^{1} \nabla^{1}_{12} X_t \\
W_t &=  \nabla^{1}_{12} (X_t - X_{t-1}) \\
W_t &=  \nabla^{1}_{12} (X_t) -  \nabla^{1}_{12} (X_{t-1}) \\
W_t &=   (X_t - X_{t-12}) -   (X_{t-1} - X_{t- 12 -1}) \\
W_t &=   X_t - X_{t-1} - X_{t-12} +  X_{t- 13}
\end{aligned}
$$
Now, we expand $\phi(B)$, understanding that $\Phi(B) = 1$ since $P = 0$.
$$
\phi(B) = (1 - \alpha_1B - \alpha_2B^2)
$$

Then we put this together as:

$$
{\varphi}^{\oplus}(B)X_t = (1 - \alpha_1B - \alpha_2B^2)(X_t - X_{t-1} - X_{t-12} +  X_{t- 13})
$$
We can expand this briefly (but not entirely) to find $p$ for ${\varphi}^{\oplus}(B)$.
$$
\begin{aligned}
{\varphi}^{\oplus}(B)X_t &= (1 - \alpha_1B - \alpha_2B^2)(X_t - X_{t-1} - X_{t-12} +  X_{t- 13}) \\
{\varphi}^{\oplus}(B)X_t &= X_t - \alpha_1BX_t - \alpha_2B^2X_t - \dots - \alpha_1BX_{t- 13} -\alpha_2B^2X_{t- 13} \\
{\varphi}^{\oplus}(B)X_t &= X_t - \alpha_1X_{t-1} - \alpha_2X_{t-2} - \dots - \alpha_1 X_{t- 14} -\alpha_2 X_{t- 15}
\end{aligned}
$$
Hence, from the final term of ${\varphi}^{\oplus}(B)X_t$, we can conclude that $p = 15$.

We can similarly determine the order $q$ of the right-hand side of the equation.

$$\text{RHS} = \theta(B) \Theta(B^s)Z_t
$$
In this situation, both $q$ and $Q$ are nonzero. We'll expand each term independently, then consider their product.
$$\theta(B) = (1+ \beta B), \hspace{ 0.25cm} \Theta(B^s) = (1 + \tilde{\beta} B^{12})$$
Hence, we can expand the AR piece of the model to be:
$$\begin{aligned}
\theta^{\oplus}(B) &= (1+ \beta B)(1 + \tilde{\beta} B^{12})Z_t \\
\theta^{\oplus}(B) &= (1+ \beta B)(1 + \tilde{\beta} B^{12})Z_t \\
\theta^{\oplus}(B) &=(1 + \beta B + \tilde{\beta}B^{12} + \beta\tilde{\beta}BB^{12}) \\
\theta^{\oplus}(B) &=Z_{t} + \beta Z_{t-1} + \tilde{\beta}Z_{t-12} + \beta\tilde{\beta}Z_{t-13}
\end{aligned}$$
Hence, from the final term of ${\theta}^{\oplus}(B)Z_t$, we can conclude that $q = 13$.

Therefore, we can conclude that the SARIMA model can decompose into an ARMA$(p = 15, q = 13)$ model under this decomposition. $\square$